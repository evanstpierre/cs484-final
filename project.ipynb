{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Number 5\n",
    "\n",
    "##### Harry Denell (hdenell@uwaterloo.ca) and Evan St. Pierre (e3stpier@uwaterloo.ca)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Team Member and Contribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Libraries\n",
    "\n",
    "\n",
    "| **Libraries**      | **Explanation**                                                                                  |\n",
    "|---------------------|--------------------------------------------------------------------------------------------------|\n",
    "| **NumPy**           | A powerful library for numerical computing in Python, providing support for arrays, matrices, and mathematical functions. |\n",
    "| **Pandas**          | Used for data manipulation and analysis; provides data structures like DataFrames for handling structured data. |\n",
    "| **Matplotlib**      | A 2D plotting library for creating static, interactive, and animated visualizations in Python.  |\n",
    "| **Seaborn**         | Built on Matplotlib, Seaborn provides an easy-to-use interface for creating informative and attractive statistical graphics. |\n",
    "| **Scikit-learn**    | A library for machine learning in Python, offering tools for classification, regression, clustering, and more. |\n",
    "| **TensorFlow**      | An open-source library for deep learning and machine learning, widely used for building neural networks. |\n",
    "| **PyTorch**         | Another deep learning framework, known for its flexibility and dynamic computation graph.         |\n",
    "| **OpenCV**          | A library for computer vision and image processing tasks such as object detection and image transformation. |\n",
    "| **Beautiful Soup**  | A library for web scraping, used to parse HTML and XML documents and extract data.                |\n",
    "| **Flask**           | A lightweight web framework in Python for building web applications and APIs.                    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Libraries\n",
    "import random\n",
    "import math\n",
    "import numbers\n",
    "import platform\n",
    "import copy\n",
    "\n",
    "# Importing essential libraries for basic image manipulations.\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image, ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# We import some of the main PyTorch and TorchVision libraries used for H\n",
    "# Detailed installation instructions are here: https://pytorch.org/get-st\n",
    "# That web site should help you to select the right 'conda install' comma\n",
    "# In particular, select the right version of CUDA. Note that prior to ins\n",
    "# install the latest driver for your GPU and CUDA (9.2 or 10.1), assuming\n",
    "# For more information about pytorch refer to\n",
    "# https://pytorch.org/docs/stable/nn.functional.html\n",
    "# https://pytorch.org/docs/stable/data.html.\n",
    "# and https://pytorch.org/docs/stable/torchvision/transforms.html\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as tF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is best to start with USE_GPU = False (implying CPU). Switch USE_GPU\n",
    "# we strongly recommend to wait until you are absolutely sure your CPU-ba\n",
    "USE_GPU = False\n",
    "\n",
    "if USE_GPU:\n",
    "    device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 1: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# Define device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \n",
    "                      (\"cuda\" if torch.cuda.is_available() else \"cpu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Data Preparation\n",
    "  - Load CIFAR-10 dataset with transformations.\n",
    "  - Split training data into labeled and unlabeled subsets.\n",
    "  - Create dataloaders for labeled, unlabeled, and test data.\n",
    "\n",
    "Step 2: labeled_labeled_model Definition\n",
    "  - Define a CNN with:\n",
    "    - Feature extraction layers.\n",
    "    - Fully connected classification head.\n",
    "  - Ensure labeled_labeled_model outputs:\n",
    "    - Features for clustering.\n",
    "    - Logits for classification.\n",
    "\n",
    "Step 3: Loss Functions\n",
    "  - Supervised Loss: Cross-entropy on labeled data.\n",
    "  - Unsupervised Loss:\n",
    "    - Perform K-Means on features of unlabeled data.\n",
    "    - Compute clustering loss (e.g., mean distance to cluster centers).\n",
    "\n",
    "Step 4: Training Loop\n",
    "  For each epoch:\n",
    "    - Train on Labeled Data (Supervised Step):\n",
    "      - Load labeled batch.\n",
    "      - Pass batch through the labeled_labeled_model.\n",
    "      - Compute cross-entropy loss.\n",
    "      - Backpropagate and update labeled_labeled_model.\n",
    "\n",
    "    - Train on Unlabeled Data (Unsupervised Step):\n",
    "      - Extract features for all unlabeled data.\n",
    "      - Perform K-Means on features.\n",
    "      - For each batch of unlabeled data:\n",
    "        - Compute clustering loss.\n",
    "        - Backpropagate and update labeled_labeled_model.\n",
    "\n",
    "    - Log losses and progress.\n",
    "\n",
    "Step 5: Testing and Evaluation\n",
    "  - Evaluate on test set:\n",
    "    - Compute accuracy using predicted labels.\n",
    "  - Analyze performance at different labeled/unlabeled splits.\n",
    "\n",
    "Step 6: Results Analysis\n",
    "  - Plot training losses.\n",
    "  - Visualize feature clustering with t-SNE.\n",
    "  - Discuss how unlabeled data improved the labeled_labeled_model's performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Data Preparation\n",
    "  - Load CIFAR-10 dataset with transformations. Use pre-defined transformations\n",
    "  - Split training data into labeled and unlabeled subsets.\n",
    "  - Create dataloaders for labeled, unlabeled, and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize RGB channels\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset (original training set only)\n",
    "full_trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "# Split into 80% train and 20% validation\n",
    "train_size = int(0.8 * len(full_trainset))\n",
    "val_size = len(full_trainset) - train_size\n",
    "trainset, valset = random_split(full_trainset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders for training and validation\n",
    "trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "valloader = DataLoader(valset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load CIFAR-10 test set (unchanged)\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform\n",
    ")\n",
    "testloader = DataLoader(testset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_ratio(data, labels, ratio, seed=42):\n",
    "    M = int(len(data) * ratio)  # Calculate number of labeled samples\n",
    "    return split_labeled_unlabeled(data, labels, M, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNN(nn.Module):\n",
    "#     def __init__(self, num_classes=10):\n",
    "#         super(CNN, self).__init__()\n",
    "#         # Feature Extractor\n",
    "#         self.feature_extractor = nn.Sequential(\n",
    "#             nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2, 2),  # Downsample\n",
    "#             nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2, 2)  # Downsample\n",
    "#         )\n",
    "#         # Classifier\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Flatten(),\n",
    "#             nn.Linear(128 * 8 * 8, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(256, num_classes)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         features = self.feature_extractor(x)\n",
    "#         logits = self.classifier(features)\n",
    "#         return features, logits\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1,1))  # [N,128,1,1]\n",
    "        )\n",
    "        self.classifier_head = nn.Linear(128, num_classes)\n",
    "        \n",
    "    def feature_extractor(self, x):\n",
    "        x = self.conv(x)           # shape: [N, 128, 1, 1]\n",
    "        x = x.view(x.size(0), -1)  # Flatten to [N, 128]\n",
    "        return x\n",
    "    \n",
    "    def classifier(self, features):\n",
    "        return self.classifier_head(features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        f = self.feature_extractor(x)\n",
    "        logits = self.classifier(f)\n",
    "        return f, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: labeled_labeled_model Definition\n",
    "  - Define a CNN with:\n",
    "    - Feature extraction layers.\n",
    "    - Fully connected classification head.\n",
    "  - Ensure labeled_labeled_model outputs:\n",
    "    - Features for clustering.\n",
    "    - Logits for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # first implementation of simple CNN for \n",
    "# class CNN(nn.Module):\n",
    "#     def __init__(self, num_classes=10):\n",
    "#         super(CNN, self).__init__()\n",
    "\n",
    "#         # Feature Extractor?\n",
    "        \n",
    "#         # Output: 32x32x32\n",
    "#         self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)  \n",
    "\n",
    "#         # Output: 64x32x32\n",
    "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)  \n",
    "\n",
    "#         # Output: Downsample to 64x16x16\n",
    "#         self.pool = nn.MaxPool2d(2, 2)  \n",
    "\n",
    "#         # Output: 128x16x16\n",
    "#         self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1) \n",
    "\n",
    "#         # Output: Downsample to 128x8x8\n",
    "#         self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "#         # Fully Connected Layers (Classification Head)\n",
    "#         self.fc1 = nn.Linear(128 * 8 * 8, 256)  # Flattened size depends on input resolution\n",
    "#         self.fc2 = nn.Linear(256, num_classes)  # Final classification layer\n",
    "\n",
    "#     def forward(self, x):\n",
    "\n",
    "#         # Convolutional Layers (Feature Extractor)\n",
    "\n",
    "#         x = F.relu(self.conv1(x))  # Conv1 + ReLU\n",
    "\n",
    "#         x = F.relu(self.conv2(x))  # Conv2 + ReLU\n",
    "\n",
    "#         x = self.pool(x)  # MaxPooling\n",
    "\n",
    "#         x = F.relu(self.conv3(x))  # Conv3 + ReLU\n",
    "        \n",
    "#         x = self.pool2(x)  # MaxPooling\n",
    "\n",
    "#         # Flatten for Fully Connected Layers\n",
    "#         features = x.view(x.size(0), -1)  # Flatten features for clustering\n",
    "#         x = F.relu(self.fc1(features))  # Fully Connected Layer 1\n",
    "#         logits = self.fc2(x)  # Fully Connected Layer 2 (Logits)\n",
    "\n",
    "#         # Return both features and logits\n",
    "#         return features, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train dataset with all labels present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n",
      "Epoch 1/10, Loss: 1.6729\n",
      "Epoch 2/10, Loss: 1.3444\n",
      "Epoch 3/10, Loss: 1.1914\n",
      "Epoch 4/10, Loss: 1.0838\n",
      "Epoch 5/10, Loss: 1.0181\n",
      "Epoch 6/10, Loss: 0.9594\n",
      "Epoch 7/10, Loss: 0.9121\n",
      "Epoch 8/10, Loss: 0.8671\n",
      "Epoch 9/10, Loss: 0.8327\n",
      "Epoch 10/10, Loss: 0.7939\n",
      "Test Accuracy: 71.04%\n"
     ]
    }
   ],
   "source": [
    "# Instantiate labeled_labeled_model, define loss, and optimizer\n",
    "labeled_model = CNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(labeled_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    labeled_model.train()  # Set labeled_model to training mode\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in trainloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to device\n",
    "\n",
    "        # Forward pass\n",
    "        features, logits = labeled_model(inputs)  # Unpack the tuple\n",
    "        loss = criterion(logits, labels)  # Use logits for loss computation\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(trainloader):.4f}\")\n",
    "\n",
    "# Evaluation loop\n",
    "labeled_model.eval()  # Set labeled_model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in testloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        features, logits = labeled_model(inputs)  # Unpack the tuple returned by the labeled_model\n",
    "        _, predicted = torch.max(logits, 1)  # Use logits for prediction\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use t-SNE or PCA to show clusters in the feature space ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take in argument based on which we want to use, lets use c_e for now\n",
    "def supervised_loss(logits, labels):\n",
    "    return F.cross_entropy(logits, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled size: 10000, Unlabeled size: 40000\n",
      "K-Means will be updated every 4 epochs based on ratio=4.00.\n",
      "Using device: mps\n",
      "Updated K-Means at epoch 1\n",
      "Epoch 1/10: Supervised Loss = 1.9655, Cluster Loss = 0.0798\n",
      "Epoch 2/10: Supervised Loss = 1.6614, Cluster Loss = 0.1003\n",
      "Epoch 3/10: Supervised Loss = 1.5418, Cluster Loss = 0.0972\n",
      "Epoch 4/10: Supervised Loss = 1.4420, Cluster Loss = 0.0944\n",
      "Updated K-Means at epoch 5\n",
      "Epoch 5/10: Supervised Loss = 1.3515, Cluster Loss = 0.0353\n",
      "Epoch 6/10: Supervised Loss = 1.2956, Cluster Loss = 0.0383\n",
      "Epoch 7/10: Supervised Loss = 1.2390, Cluster Loss = 0.0427\n",
      "Epoch 8/10: Supervised Loss = 1.1889, Cluster Loss = 0.0439\n",
      "Updated K-Means at epoch 9\n",
      "Epoch 9/10: Supervised Loss = 1.1464, Cluster Loss = 0.0411\n",
      "Epoch 10/10: Supervised Loss = 1.0963, Cluster Loss = 0.0437\n",
      "Test Accuracy: 60.01%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "full_trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "LABEL_SIZE = 0.2\n",
    "\n",
    "# Split into labeled and unlabeled datasets (20% labeled)\n",
    "labeled_size = int(LABEL_SIZE * len(full_trainset))\n",
    "unlabeled_size = len(full_trainset) - labeled_size\n",
    "labeled_set, unlabeled_set = random_split(full_trainset, [labeled_size, unlabeled_size])\n",
    "\n",
    "print(f\"Labeled size: {labeled_size}, Unlabeled size: {unlabeled_size}\")\n",
    "\n",
    "\n",
    "# Compute ratio\n",
    "ratio = unlabeled_size / labeled_size\n",
    "\n",
    "# A simple heuristic: if ratio is large, update K-Means less often\n",
    "# For instance, if ratio = 1 means run every epoch, if ratio=5 means run every 5 epochs\n",
    "kmeans_update_frequency = max(1, int(ratio))\n",
    "\n",
    "print(f\"K-Means will be updated every {kmeans_update_frequency} epochs based on ratio={ratio:.2f}.\")\n",
    "\n",
    "# Create DataLoaders for labeled and unlabeled datasets\n",
    "labeled_loader = DataLoader(labeled_set, batch_size=32, shuffle=True)\n",
    "unlabeled_loader = DataLoader(unlabeled_set, batch_size=32, shuffle=True)\n",
    "\n",
    "# Load CIFAR-10 test set\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = DataLoader(testset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \n",
    "                      (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "model = CNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "K = 10  # number of clusters\n",
    "lambda_cluster = 0.5\n",
    "\n",
    "cluster_centers_torch = None  # will store cluster centers\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.eval()\n",
    "    \n",
    "    # Update K-Means only every kmeans_update_frequency epochs or if we have no cluster centers yet\n",
    "    if epoch % kmeans_update_frequency == 0 or cluster_centers_torch is None:\n",
    "        # ----- Run K-Means Clustering on Unlabeled Data -----\n",
    "        all_features = []\n",
    "        with torch.no_grad():\n",
    "            for unlabeled_batch in unlabeled_loader:\n",
    "                u_inputs = unlabeled_batch[0].to(device)\n",
    "                u_features, _ = model(u_inputs)\n",
    "                all_features.append(u_features.cpu().numpy())\n",
    "        \n",
    "        all_features = np.concatenate(all_features, axis=0)  # shape: [num_unlabeled_samples, feature_dim]\n",
    "\n",
    "        # Fit KMeans\n",
    "        kmeans = KMeans(n_clusters=K, random_state=42)\n",
    "        kmeans.fit(all_features)\n",
    "        cluster_centers = kmeans.cluster_centers_\n",
    "        cluster_centers_torch = torch.tensor(cluster_centers, dtype=torch.float32, device=device)\n",
    "        print(f\"Updated K-Means at epoch {epoch+1}\")\n",
    "\n",
    "    # Reset unlabeled loader iterator for this epoch\n",
    "    unlabeled_iter = iter(unlabeled_loader)\n",
    "\n",
    "    model.train()\n",
    "    running_supervised_loss = 0.0\n",
    "    running_cluster_loss = 0.0\n",
    "    \n",
    "    for x_l, y_l in labeled_loader:\n",
    "        x_l, y_l = x_l.to(device), y_l.to(device)\n",
    "        \n",
    "        # Get an unlabeled batch\n",
    "        try:\n",
    "            x_u, _ = next(unlabeled_iter)\n",
    "        except StopIteration:\n",
    "            unlabeled_iter = iter(unlabeled_loader)\n",
    "            x_u, _ = next(unlabeled_iter)\n",
    "        \n",
    "        x_u = x_u.to(device)\n",
    "        \n",
    "        # Compute supervised loss\n",
    "        f_l, logits_l = model(x_l)\n",
    "        supervised_loss = criterion(logits_l, y_l)\n",
    "        \n",
    "        # Compute cluster loss for the unlabeled batch:\n",
    "        f_u, _ = model(x_u)\n",
    "        \n",
    "        # Assign clusters by finding nearest center\n",
    "        with torch.no_grad():\n",
    "            dists = torch.cdist(f_u, cluster_centers_torch)\n",
    "            assignments = dists.argmin(dim=1)\n",
    "        \n",
    "        assigned_centers = cluster_centers_torch[assignments]\n",
    "        cluster_loss = ((f_u - assigned_centers)**2).mean()\n",
    "        \n",
    "        # Combine losses and backprop\n",
    "        total_loss = supervised_loss + lambda_cluster * cluster_loss\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_supervised_loss += supervised_loss.item()\n",
    "        running_cluster_loss += cluster_loss.item()\n",
    "    \n",
    "    avg_supervised_loss = running_supervised_loss / len(labeled_loader)\n",
    "    avg_cluster_loss = running_cluster_loss / len(labeled_loader)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: \"\n",
    "          f\"Supervised Loss = {avg_supervised_loss:.4f}, \"\n",
    "          f\"Cluster Loss = {avg_cluster_loss:.4f}\")\n",
    "\n",
    "# ----- Evaluation -----\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in testloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        _, logits = model(inputs)\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each epoch:\n",
    "    1. Supervised Phase:\n",
    "        - For each batch in labeled_loader:\n",
    "            a. Pass images through the labeled_labeled_model to get predictions (logits).\n",
    "            b. Compute supervised loss (cross-entropy).\n",
    "            c. Backpropagate and update labeled_labeled_model weights.\n",
    "\n",
    "    2. Unsupervised Phase:\n",
    "        a. Extract features for all images in unlabeled_loader.\n",
    "        b. Perform clustering (e.g., K-Means) on the features.\n",
    "        c. For each batch in unlabeled_loader:\n",
    "            - Compute clustering loss (distance to cluster centers).\n",
    "            - Backpropagate and update the feature extractor.\n",
    "\n",
    "    Log both supervised and unsupervised losses.\n",
    "    Evaluate on the test set to track performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
