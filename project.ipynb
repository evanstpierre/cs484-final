{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Number 5\n",
    "\n",
    "##### Harry Denell (hdenell@uwaterloo.ca) and Evan St. Pierre (e3stpier@uwaterloo.ca)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Team Member and Contribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Libraries\n",
    "\n",
    "\n",
    "| **Libraries**      | **Explanation**                                                                                  |\n",
    "|---------------------|--------------------------------------------------------------------------------------------------|\n",
    "| **NumPy**           | A powerful library for numerical computing in Python, providing support for arrays, matrices, and mathematical functions. |\n",
    "| **Pandas**          | Used for data manipulation and analysis; provides data structures like DataFrames for handling structured data. |\n",
    "| **Matplotlib**      | A 2D plotting library for creating static, interactive, and animated visualizations in Python.  |\n",
    "| **Seaborn**         | Built on Matplotlib, Seaborn provides an easy-to-use interface for creating informative and attractive statistical graphics. |\n",
    "| **Scikit-learn**    | A library for machine learning in Python, offering tools for classification, regression, clustering, and more. |\n",
    "| **TensorFlow**      | An open-source library for deep learning and machine learning, widely used for building neural networks. |\n",
    "| **PyTorch**         | Another deep learning framework, known for its flexibility and dynamic computation graph.         |\n",
    "| **OpenCV**          | A library for computer vision and image processing tasks such as object detection and image transformation. |\n",
    "| **Beautiful Soup**  | A library for web scraping, used to parse HTML and XML documents and extract data.                |\n",
    "| **Flask**           | A lightweight web framework in Python for building web applications and APIs.                    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Libraries\n",
    "import random\n",
    "import math\n",
    "import numbers\n",
    "import platform\n",
    "import copy\n",
    "\n",
    "# Importing essential libraries for basic image manipulations.\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image, ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# We import some of the main PyTorch and TorchVision libraries used for H\n",
    "# Detailed installation instructions are here: https://pytorch.org/get-st\n",
    "# That web site should help you to select the right 'conda install' comma\n",
    "# In particular, select the right version of CUDA. Note that prior to ins\n",
    "# install the latest driver for your GPU and CUDA (9.2 or 10.1), assuming\n",
    "# For more information about pytorch refer to\n",
    "# https://pytorch.org/docs/stable/nn.functional.html\n",
    "# https://pytorch.org/docs/stable/data.html.\n",
    "# and https://pytorch.org/docs/stable/torchvision/transforms.html\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as tF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is best to start with USE_GPU = False (implying CPU). Switch USE_GPU\n",
    "# we strongly recommend to wait until you are absolutely sure your CPU-ba\n",
    "USE_GPU = False\n",
    "\n",
    "if USE_GPU:\n",
    "    device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 1: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import torch.optim as optim\n",
    "print(torch.backends.mps.is_available())  # Should return True if supported\n",
    "print(torch.backends.mps.is_built())     # Should return True if correctly built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (1022393054.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[29], line 8\u001b[0;36m\u001b[0m\n\u001b[0;31m    devices = tf.4config.list_physical_devices()\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")\n",
    "\n",
    "devices = tf.4config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Data Preparation\n",
    "  - Load CIFAR-10 dataset with transformations.\n",
    "  - Split training data into labeled and unlabeled subsets.\n",
    "  - Create dataloaders for labeled, unlabeled, and test data.\n",
    "\n",
    "Step 2: labeled_labeled_model Definition\n",
    "  - Define a CNN with:\n",
    "    - Feature extraction layers.\n",
    "    - Fully connected classification head.\n",
    "  - Ensure labeled_labeled_model outputs:\n",
    "    - Features for clustering.\n",
    "    - Logits for classification.\n",
    "\n",
    "Step 3: Loss Functions\n",
    "  - Supervised Loss: Cross-entropy on labeled data.\n",
    "  - Unsupervised Loss:\n",
    "    - Perform K-Means on features of unlabeled data.\n",
    "    - Compute clustering loss (e.g., mean distance to cluster centers).\n",
    "\n",
    "Step 4: Training Loop\n",
    "  For each epoch:\n",
    "    - Train on Labeled Data (Supervised Step):\n",
    "      - Load labeled batch.\n",
    "      - Pass batch through the labeled_labeled_model.\n",
    "      - Compute cross-entropy loss.\n",
    "      - Backpropagate and update labeled_labeled_model.\n",
    "\n",
    "    - Train on Unlabeled Data (Unsupervised Step):\n",
    "      - Extract features for all unlabeled data.\n",
    "      - Perform K-Means on features.\n",
    "      - For each batch of unlabeled data:\n",
    "        - Compute clustering loss.\n",
    "        - Backpropagate and update labeled_labeled_model.\n",
    "\n",
    "    - Log losses and progress.\n",
    "\n",
    "Step 5: Testing and Evaluation\n",
    "  - Evaluate on test set:\n",
    "    - Compute accuracy using predicted labels.\n",
    "  - Analyze performance at different labeled/unlabeled splits.\n",
    "\n",
    "Step 6: Results Analysis\n",
    "  - Plot training losses.\n",
    "  - Visualize feature clustering with t-SNE.\n",
    "  - Discuss how unlabeled data improved the labeled_labeled_model's performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Data Preparation\n",
    "  - Load CIFAR-10 dataset with transformations. Use pre-defined transformations\n",
    "  - Split training data into labeled and unlabeled subsets.\n",
    "  - Create dataloaders for labeled, unlabeled, and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize RGB channels\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset (original training set only)\n",
    "full_trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "# Split into 80% train and 20% validation\n",
    "train_size = int(0.8 * len(full_trainset))\n",
    "val_size = len(full_trainset) - train_size\n",
    "trainset, valset = random_split(full_trainset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders for training and validation\n",
    "trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "valloader = DataLoader(valset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load CIFAR-10 test set (unchanged)\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform\n",
    ")\n",
    "testloader = DataLoader(testset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_ratio(data, labels, ratio, seed=42):\n",
    "    M = int(len(data) * ratio)  # Calculate number of labeled samples\n",
    "    return split_labeled_unlabeled(data, labels, M, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: labeled_labeled_model Definition\n",
    "  - Define a CNN with:\n",
    "    - Feature extraction layers.\n",
    "    - Fully connected classification head.\n",
    "  - Ensure labeled_labeled_model outputs:\n",
    "    - Features for clustering.\n",
    "    - Logits for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first implementation of simple CNN for \n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # Feature Extractor?\n",
    "        \n",
    "        # Output: 32x32x32\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)  \n",
    "\n",
    "        # Output: 64x32x32\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)  \n",
    "\n",
    "        # Output: Downsample to 64x16x16\n",
    "        self.pool = nn.MaxPool2d(2, 2)  \n",
    "\n",
    "        # Output: 128x16x16\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1) \n",
    "\n",
    "        # Output: Downsample to 128x8x8\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Fully Connected Layers (Classification Head)\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, 256)  # Flattened size depends on input resolution\n",
    "        self.fc2 = nn.Linear(256, num_classes)  # Final classification layer\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Convolutional Layers (Feature Extractor)\n",
    "\n",
    "        x = F.relu(self.conv1(x))  # Conv1 + ReLU\n",
    "\n",
    "        x = F.relu(self.conv2(x))  # Conv2 + ReLU\n",
    "\n",
    "        x = self.pool(x)  # MaxPooling\n",
    "\n",
    "        x = F.relu(self.conv3(x))  # Conv3 + ReLU\n",
    "        \n",
    "        x = self.pool2(x)  # MaxPooling\n",
    "\n",
    "        # Flatten for Fully Connected Layers\n",
    "        features = x.view(x.size(0), -1)  # Flatten features for clustering\n",
    "        x = F.relu(self.fc1(features))  # Fully Connected Layer 1\n",
    "        logits = self.fc2(x)  # Fully Connected Layer 2 (Logits)\n",
    "\n",
    "        # Return both features and logits\n",
    "        return features, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: torch.Size([4, 8192])\n",
      "Logits shape: torch.Size([4, 10])\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the CNN\n",
    "labeled_labeled_model = CNN(num_classes=10)\n",
    "\n",
    "# Create a batch of random input images\n",
    "# Shape: [batch_size, channels, height, width] -> [4, 3, 32, 32]\n",
    "batch_size = 4\n",
    "dummy_images = torch.randn(batch_size, 3, 32, 32)\n",
    "\n",
    "# Forward pass\n",
    "features, logits = labeled_labeled_model(dummy_images)\n",
    "\n",
    "# Print outputs\n",
    "print(\"Features shape:\", features.shape)  # Expected: [4, 128 * 8 * 8]\n",
    "print(\"Logits shape:\", logits.shape)      # Expected: [4, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train dataset with all labels present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n",
      "Epoch 1/10, Loss: 1.2658\n",
      "Epoch 2/10, Loss: 0.8153\n",
      "Epoch 3/10, Loss: 0.5973\n",
      "Epoch 4/10, Loss: 0.4138\n",
      "Epoch 5/10, Loss: 0.2582\n",
      "Epoch 6/10, Loss: 0.1600\n",
      "Epoch 7/10, Loss: 0.1164\n",
      "Epoch 8/10, Loss: 0.0956\n",
      "Epoch 9/10, Loss: 0.0888\n",
      "Epoch 10/10, Loss: 0.0760\n",
      "Test Accuracy: 72.18%\n"
     ]
    }
   ],
   "source": [
    "# Instantiate labeled_labeled_model, define loss, and optimizer\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "print(device)\n",
    "labeled_model = CNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(labeled_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    labeled_model.train()  # Set labeled_model to training mode\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in trainloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to device\n",
    "\n",
    "        # Forward pass\n",
    "        features, logits = labeled_model(inputs)  # Unpack the tuple\n",
    "        loss = criterion(logits, labels)  # Use logits for loss computation\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(trainloader):.4f}\")\n",
    "\n",
    "# Evaluation loop\n",
    "labeled_model.eval()  # Set labeled_model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in testloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        features, logits = labeled_model(inputs)  # Unpack the tuple returned by the labeled_model\n",
    "        _, predicted = torch.max(logits, 1)  # Use logits for prediction\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        # Feature Extractor\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # Downsample\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)  # Downsample\n",
    "        )\n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 8 * 8, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        logits = self.classifier(features)\n",
    "        return features, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 4. KMeans expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 91\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Update KMeans clustering periodically\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m kmeans_update_frequency \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 91\u001b[0m     kmeans\u001b[38;5;241m.\u001b[39mfit(all_features)  \u001b[38;5;66;03m# Update cluster centers\u001b[39;00m\n\u001b[1;32m     92\u001b[0m cluster_assignments \u001b[38;5;241m=\u001b[39m kmeans\u001b[38;5;241m.\u001b[39mpredict(all_features)\n\u001b[1;32m     93\u001b[0m cluster_centers \u001b[38;5;241m=\u001b[39m kmeans\u001b[38;5;241m.\u001b[39mcluster_centers_\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/cluster/_kmeans.py:1481\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1453\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1454\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1455\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute k-means clustering.\u001b[39;00m\n\u001b[1;32m   1456\u001b[0m \n\u001b[1;32m   1457\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1479\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1480\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1481\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m   1482\u001b[0m         X,\n\u001b[1;32m   1483\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1484\u001b[0m         dtype\u001b[38;5;241m=\u001b[39m[np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32],\n\u001b[1;32m   1485\u001b[0m         order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1486\u001b[0m         copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy_x,\n\u001b[1;32m   1487\u001b[0m         accept_large_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1488\u001b[0m     )\n\u001b[1;32m   1490\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params_vs_input(X)\n\u001b[1;32m   1492\u001b[0m     random_state \u001b[38;5;241m=\u001b[39m check_random_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:1043\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1039\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1041\u001b[0m     )\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nd \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m-> 1043\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1044\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1045\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1046\u001b[0m     )\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m   1049\u001b[0m     _assert_all_finite(\n\u001b[1;32m   1050\u001b[0m         array,\n\u001b[1;32m   1051\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m   1052\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m   1053\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1054\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with dim 4. KMeans expected <= 2."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "full_trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Split into labeled and unlabeled datasets\n",
    "labeled_size = int(0.2 * len(full_trainset))  # 20% labeled\n",
    "unlabeled_size = len(full_trainset) - labeled_size\n",
    "labeled_set, unlabeled_set = random_split(full_trainset, [labeled_size, unlabeled_size])\n",
    "\n",
    "# Create DataLoaders for labeled and unlabeled datasets\n",
    "labeled_loader = DataLoader(labeled_set, batch_size=32, shuffle=True)\n",
    "unlabeled_loader = DataLoader(unlabeled_set, batch_size=32, shuffle=True)\n",
    "\n",
    "# Load CIFAR-10 test set\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = DataLoader(testset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# CNN model\n",
    "# ...\n",
    "\n",
    "# Instantiate the model\n",
    "model = CNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop with labeled and unlabeled data\n",
    "num_epochs = 10\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)  # Initialize KMeans\n",
    "kmeans_update_frequency = 5  # Update KMeans every 5 epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # ----- Step 1: Supervised Training -----\n",
    "\n",
    "    model.train()  # Set model to training mode\n",
    "    running_supervised_loss = 0.0\n",
    "\n",
    "    for labeled_batch in labeled_loader:\n",
    "        inputs, labels = labeled_batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Extract features and compute logits\n",
    "        features = model.feature_extractor(inputs)\n",
    "        logits = model.classifier(features)\n",
    "\n",
    "        # Compute supervised loss\n",
    "        supervised_loss = criterion(logits, labels)\n",
    "\n",
    "        # Backpropagate supervised loss\n",
    "        optimizer.zero_grad()\n",
    "        supervised_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_supervised_loss += supervised_loss.item()\n",
    "\n",
    "    # ----- Step 2: Clustering on Unlabeled Data -----\n",
    "    model.eval()  # Set model to evaluation mode (no dropout, batchnorm frozen)\n",
    "    all_features = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for feature extraction\n",
    "        for unlabeled_batch in unlabeled_loader:\n",
    "            inputs = unlabeled_batch[0]  # Extract the tensor from the list\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            # Extract features\n",
    "            features = model.feature_extractor(inputs).detach()\n",
    "            all_features.append(features.cpu().numpy())\n",
    "\n",
    "    # Concatenate all extracted features\n",
    "    all_features = np.concatenate(all_features, axis=0)\n",
    "\n",
    "    # Update KMeans clustering periodically\n",
    "    if epoch % kmeans_update_frequency == 0:\n",
    "        print(all_features.shape)\n",
    "        kmeans.fit(all_features)  # Update cluster centers\n",
    "    cluster_assignments = kmeans.predict(all_features)\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "\n",
    "    # Compute clustering loss\n",
    "    clustering_loss = 0\n",
    "    for i, feature in enumerate(all_features):\n",
    "        assigned_center = torch.tensor(cluster_centers[cluster_assignments[i]], device=device)\n",
    "        feature_tensor = torch.tensor(feature, device=device)\n",
    "        clustering_loss += torch.norm(feature_tensor - assigned_center) ** 2\n",
    "\n",
    "    clustering_loss = clustering_loss / len(unlabeled_loader)\n",
    "\n",
    "    # Backpropagate clustering loss (only for feature extractor)\n",
    "    optimizer.zero_grad()\n",
    "    clustering_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # ----- Logging -----\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}: \"\n",
    "          f\"Supervised Loss = {running_supervised_loss / len(labeled_loader):.4f}, \"\n",
    "          f\"Clustering Loss = {clustering_loss.item():.4f}\")\n",
    "# ----- Evaluation -----\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in testloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        _, logits = model(inputs)\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use t-SNE or PCA to show clusters in the feature space ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take in argument based on which we want to use, lets use c_e for now\n",
    "def supervised_loss(logits, labels):\n",
    "    return F.cross_entropy(logits, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each epoch:\n",
    "    1. Supervised Phase:\n",
    "        - For each batch in labeled_loader:\n",
    "            a. Pass images through the labeled_labeled_model to get predictions (logits).\n",
    "            b. Compute supervised loss (cross-entropy).\n",
    "            c. Backpropagate and update labeled_labeled_model weights.\n",
    "\n",
    "    2. Unsupervised Phase:\n",
    "        a. Extract features for all images in unlabeled_loader.\n",
    "        b. Perform clustering (e.g., K-Means) on the features.\n",
    "        c. For each batch in unlabeled_loader:\n",
    "            - Compute clustering loss (distance to cluster centers).\n",
    "            - Backpropagate and update the feature extractor.\n",
    "\n",
    "    Log both supervised and unsupervised losses.\n",
    "    Evaluate on the test set to track performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
