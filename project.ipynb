{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Number 5\n",
    "\n",
    "##### Harry Denell (hdenell@uwaterloo.ca) and Evan St. Pierre (e3stpier@uwaterloo.ca)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Team Member and Contribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Libraries\n",
    "\n",
    "\n",
    "| **Libraries**      | **Explanation**                                                                                  |\n",
    "|---------------------|--------------------------------------------------------------------------------------------------|\n",
    "| **NumPy**           | A powerful library for numerical computing in Python, providing support for arrays, matrices, and mathematical functions. |\n",
    "| **Pandas**          | Used for data manipulation and analysis; provides data structures like DataFrames for handling structured data. |\n",
    "| **Matplotlib**      | A 2D plotting library for creating static, interactive, and animated visualizations in Python.  |\n",
    "| **Seaborn**         | Built on Matplotlib, Seaborn provides an easy-to-use interface for creating informative and attractive statistical graphics. |\n",
    "| **Scikit-learn**    | A library for machine learning in Python, offering tools for classification, regression, clustering, and more. |\n",
    "| **TensorFlow**      | An open-source library for deep learning and machine learning, widely used for building neural networks. |\n",
    "| **PyTorch**         | Another deep learning framework, known for its flexibility and dynamic computation graph.         |\n",
    "| **OpenCV**          | A library for computer vision and image processing tasks such as object detection and image transformation. |\n",
    "| **Beautiful Soup**  | A library for web scraping, used to parse HTML and XML documents and extract data.                |\n",
    "| **Flask**           | A lightweight web framework in Python for building web applications and APIs.                    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Libraries\n",
    "import random\n",
    "import math\n",
    "import numbers\n",
    "import platform\n",
    "import copy\n",
    "\n",
    "# Importing essential libraries for basic image manipulations.\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image, ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# We import some of the main PyTorch and TorchVision libraries used for H\n",
    "# Detailed installation instructions are here: https://pytorch.org/get-st\n",
    "# That web site should help you to select the right 'conda install' comma\n",
    "# In particular, select the right version of CUDA. Note that prior to ins\n",
    "# install the latest driver for your GPU and CUDA (9.2 or 10.1), assuming\n",
    "# For more information about pytorch refer to\n",
    "# https://pytorch.org/docs/stable/nn.functional.html\n",
    "# https://pytorch.org/docs/stable/data.html.\n",
    "# and https://pytorch.org/docs/stable/torchvision/transforms.html\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as tF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is best to start with USE_GPU = False (implying CPU). Switch USE_GPU\n",
    "# we strongly recommend to wait until you are absolutely sure your CPU-ba\n",
    "USE_GPU = False\n",
    "\n",
    "if USE_GPU:\n",
    "    device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 1: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# Define device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \n",
    "                      (\"cuda\" if torch.cuda.is_available() else \"cpu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Data Preparation\n",
    "  - Load CIFAR-10 dataset with transformations.\n",
    "  - Split training data into labeled and unlabeled subsets.\n",
    "  - Create dataloaders for labeled, unlabeled, and test data.\n",
    "\n",
    "Step 2: labeled_labeled_model Definition\n",
    "  - Define a CNN with:\n",
    "    - Feature extraction layers.\n",
    "    - Fully connected classification head.\n",
    "  - Ensure labeled_labeled_model outputs:\n",
    "    - Features for clustering.\n",
    "    - Logits for classification.\n",
    "\n",
    "Step 3: Loss Functions\n",
    "  - Supervised Loss: Cross-entropy on labeled data.\n",
    "  - Unsupervised Loss:\n",
    "    - Perform K-Means on features of unlabeled data.\n",
    "    - Compute clustering loss (e.g., mean distance to cluster centers).\n",
    "\n",
    "Step 4: Training Loop\n",
    "  For each epoch:\n",
    "    - Train on Labeled Data (Supervised Step):\n",
    "      - Load labeled batch.\n",
    "      - Pass batch through the labeled_labeled_model.\n",
    "      - Compute cross-entropy loss.\n",
    "      - Backpropagate and update labeled_labeled_model.\n",
    "\n",
    "    - Train on Unlabeled Data (Unsupervised Step):\n",
    "      - Extract features for all unlabeled data.\n",
    "      - Perform K-Means on features.\n",
    "      - For each batch of unlabeled data:\n",
    "        - Compute clustering loss.\n",
    "        - Backpropagate and update labeled_labeled_model.\n",
    "\n",
    "    - Log losses and progress.\n",
    "\n",
    "Step 5: Testing and Evaluation\n",
    "  - Evaluate on test set:\n",
    "    - Compute accuracy using predicted labels.\n",
    "  - Analyze performance at different labeled/unlabeled splits.\n",
    "\n",
    "Step 6: Results Analysis\n",
    "  - Plot training losses.\n",
    "  - Visualize feature clustering with t-SNE.\n",
    "  - Discuss how unlabeled data improved the labeled_labeled_model's performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Data Preparation\n",
    "  - Load CIFAR-10 dataset with transformations. Use pre-defined transformations\n",
    "  - Split training data into labeled and unlabeled subsets.\n",
    "  - Create dataloaders for labeled, unlabeled, and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize RGB channels\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset (original training set only)\n",
    "full_trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "# Split into 80% train and 20% validation\n",
    "train_size = int(0.8 * len(full_trainset))\n",
    "val_size = len(full_trainset) - train_size\n",
    "trainset, valset = random_split(full_trainset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders for training and validation\n",
    "trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "valloader = DataLoader(valset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load CIFAR-10 test set (unchanged)\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform\n",
    ")\n",
    "testloader = DataLoader(testset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_ratio(data, labels, ratio, seed=42):\n",
    "    M = int(len(data) * ratio)  # Calculate number of labeled samples\n",
    "    return split_labeled_unlabeled(data, labels, M, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNN(nn.Module):\n",
    "#     def __init__(self, num_classes=10):\n",
    "#         super(CNN, self).__init__()\n",
    "#         # Feature Extractor\n",
    "#         self.feature_extractor = nn.Sequential(\n",
    "#             nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2, 2),  # Downsample\n",
    "#             nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2, 2)  # Downsample\n",
    "#         )\n",
    "#         # Classifier\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Flatten(),\n",
    "#             nn.Linear(128 * 8 * 8, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(256, num_classes)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         features = self.feature_extractor(x)\n",
    "#         logits = self.classifier(features)\n",
    "#         return features, logits\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1,1))  # [N,128,1,1]\n",
    "        )\n",
    "        self.classifier_head = nn.Linear(128, num_classes)\n",
    "        \n",
    "    def feature_extractor(self, x):\n",
    "        x = self.conv(x)           # shape: [N, 128, 1, 1]\n",
    "        x = x.view(x.size(0), -1)  # Flatten to [N, 128]\n",
    "        return x\n",
    "    \n",
    "    def classifier(self, features):\n",
    "        return self.classifier_head(features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        f = self.feature_extractor(x)\n",
    "        logits = self.classifier(f)\n",
    "        return f, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: labeled_labeled_model Definition\n",
    "  - Define a CNN with:\n",
    "    - Feature extraction layers.\n",
    "    - Fully connected classification head.\n",
    "  - Ensure labeled_labeled_model outputs:\n",
    "    - Features for clustering.\n",
    "    - Logits for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # first implementation of simple CNN for \n",
    "# class CNN(nn.Module):\n",
    "#     def __init__(self, num_classes=10):\n",
    "#         super(CNN, self).__init__()\n",
    "\n",
    "#         # Feature Extractor?\n",
    "        \n",
    "#         # Output: 32x32x32\n",
    "#         self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)  \n",
    "\n",
    "#         # Output: 64x32x32\n",
    "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)  \n",
    "\n",
    "#         # Output: Downsample to 64x16x16\n",
    "#         self.pool = nn.MaxPool2d(2, 2)  \n",
    "\n",
    "#         # Output: 128x16x16\n",
    "#         self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1) \n",
    "\n",
    "#         # Output: Downsample to 128x8x8\n",
    "#         self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "#         # Fully Connected Layers (Classification Head)\n",
    "#         self.fc1 = nn.Linear(128 * 8 * 8, 256)  # Flattened size depends on input resolution\n",
    "#         self.fc2 = nn.Linear(256, num_classes)  # Final classification layer\n",
    "\n",
    "#     def forward(self, x):\n",
    "\n",
    "#         # Convolutional Layers (Feature Extractor)\n",
    "\n",
    "#         x = F.relu(self.conv1(x))  # Conv1 + ReLU\n",
    "\n",
    "#         x = F.relu(self.conv2(x))  # Conv2 + ReLU\n",
    "\n",
    "#         x = self.pool(x)  # MaxPooling\n",
    "\n",
    "#         x = F.relu(self.conv3(x))  # Conv3 + ReLU\n",
    "        \n",
    "#         x = self.pool2(x)  # MaxPooling\n",
    "\n",
    "#         # Flatten for Fully Connected Layers\n",
    "#         features = x.view(x.size(0), -1)  # Flatten features for clustering\n",
    "#         x = F.relu(self.fc1(features))  # Fully Connected Layer 1\n",
    "#         logits = self.fc2(x)  # Fully Connected Layer 2 (Logits)\n",
    "\n",
    "#         # Return both features and logits\n",
    "#         return features, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train dataset with all labels present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n",
      "Epoch 1/10, Loss: 1.6729\n",
      "Epoch 2/10, Loss: 1.3444\n",
      "Epoch 3/10, Loss: 1.1914\n",
      "Epoch 4/10, Loss: 1.0838\n",
      "Epoch 5/10, Loss: 1.0181\n",
      "Epoch 6/10, Loss: 0.9594\n",
      "Epoch 7/10, Loss: 0.9121\n",
      "Epoch 8/10, Loss: 0.8671\n"
     ]
    }
   ],
   "source": [
    "# Instantiate labeled_labeled_model, define loss, and optimizer\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "print(device)\n",
    "labeled_model = CNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(labeled_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    labeled_model.train()  # Set labeled_model to training mode\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in trainloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to device\n",
    "\n",
    "        # Forward pass\n",
    "        features, logits = labeled_model(inputs)  # Unpack the tuple\n",
    "        loss = criterion(logits, labels)  # Use logits for loss computation\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(trainloader):.4f}\")\n",
    "\n",
    "# Evaluation loop\n",
    "labeled_model.eval()  # Set labeled_model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in testloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        features, logits = labeled_model(inputs)  # Unpack the tuple returned by the labeled_model\n",
    "        _, predicted = torch.max(logits, 1)  # Use logits for prediction\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use t-SNE or PCA to show clusters in the feature space ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take in argument based on which we want to use, lets use c_e for now\n",
    "def supervised_loss(logits, labels):\n",
    "    return F.cross_entropy(logits, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled size: 10000, Unlabeled size: 40000\n",
      "K-Means will be updated every 4 epochs based on ratio=4.00.\n",
      "Using device: mps\n",
      "Updated K-Means at epoch 1\n",
      "Epoch 1/10: Supervised Loss = 1.6766, Cluster Loss = 0.0100\n",
      "Epoch 2/10: Supervised Loss = 1.2525, Cluster Loss = 0.0144\n",
      "Epoch 3/10: Supervised Loss = 0.9865, Cluster Loss = 0.0202\n",
      "Epoch 4/10: Supervised Loss = 0.7393, Cluster Loss = 0.0283\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 79\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Fit KMeans\u001b[39;00m\n\u001b[1;32m     78\u001b[0m kmeans \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39mK, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m---> 79\u001b[0m kmeans\u001b[38;5;241m.\u001b[39mfit(all_features)\n\u001b[1;32m     80\u001b[0m cluster_centers \u001b[38;5;241m=\u001b[39m kmeans\u001b[38;5;241m.\u001b[39mcluster_centers_\n\u001b[1;32m     81\u001b[0m cluster_centers_torch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(cluster_centers, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/cluster/_kmeans.py:1525\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1521\u001b[0m best_inertia, best_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_init):\n\u001b[1;32m   1524\u001b[0m     \u001b[38;5;66;03m# Initialize centers\u001b[39;00m\n\u001b[0;32m-> 1525\u001b[0m     centers_init \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_centroids(\n\u001b[1;32m   1526\u001b[0m         X,\n\u001b[1;32m   1527\u001b[0m         x_squared_norms\u001b[38;5;241m=\u001b[39mx_squared_norms,\n\u001b[1;32m   1528\u001b[0m         init\u001b[38;5;241m=\u001b[39minit,\n\u001b[1;32m   1529\u001b[0m         random_state\u001b[38;5;241m=\u001b[39mrandom_state,\n\u001b[1;32m   1530\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[1;32m   1531\u001b[0m     )\n\u001b[1;32m   1532\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[1;32m   1533\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitialization complete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/cluster/_kmeans.py:1021\u001b[0m, in \u001b[0;36m_BaseKMeans._init_centroids\u001b[0;34m(self, X, x_squared_norms, init, random_state, sample_weight, init_size, n_centroids)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m sample_weight[init_indices]\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(init, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m init \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk-means++\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1021\u001b[0m     centers, _ \u001b[38;5;241m=\u001b[39m _kmeans_plusplus(\n\u001b[1;32m   1022\u001b[0m         X,\n\u001b[1;32m   1023\u001b[0m         n_clusters,\n\u001b[1;32m   1024\u001b[0m         random_state\u001b[38;5;241m=\u001b[39mrandom_state,\n\u001b[1;32m   1025\u001b[0m         x_squared_norms\u001b[38;5;241m=\u001b[39mx_squared_norms,\n\u001b[1;32m   1026\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[1;32m   1027\u001b[0m     )\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(init, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m init \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1029\u001b[0m     seeds \u001b[38;5;241m=\u001b[39m random_state\u001b[38;5;241m.\u001b[39mchoice(\n\u001b[1;32m   1030\u001b[0m         n_samples,\n\u001b[1;32m   1031\u001b[0m         size\u001b[38;5;241m=\u001b[39mn_clusters,\n\u001b[1;32m   1032\u001b[0m         replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1033\u001b[0m         p\u001b[38;5;241m=\u001b[39msample_weight \u001b[38;5;241m/\u001b[39m sample_weight\u001b[38;5;241m.\u001b[39msum(),\n\u001b[1;32m   1034\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/cluster/_kmeans.py:255\u001b[0m, in \u001b[0;36m_kmeans_plusplus\u001b[0;34m(X, n_clusters, x_squared_norms, sample_weight, random_state, n_local_trials)\u001b[0m\n\u001b[1;32m    252\u001b[0m np\u001b[38;5;241m.\u001b[39mclip(candidate_ids, \u001b[38;5;28;01mNone\u001b[39;00m, closest_dist_sq\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, out\u001b[38;5;241m=\u001b[39mcandidate_ids)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;66;03m# Compute distances to center candidates\u001b[39;00m\n\u001b[0;32m--> 255\u001b[0m distance_to_candidates \u001b[38;5;241m=\u001b[39m _euclidean_distances(\n\u001b[1;32m    256\u001b[0m     X[candidate_ids], X, Y_norm_squared\u001b[38;5;241m=\u001b[39mx_squared_norms, squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    257\u001b[0m )\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# update closest distances squared and potential for each candidate\u001b[39;00m\n\u001b[1;32m    260\u001b[0m np\u001b[38;5;241m.\u001b[39mminimum(closest_dist_sq, distance_to_candidates, out\u001b[38;5;241m=\u001b[39mdistance_to_candidates)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/metrics/pairwise.py:379\u001b[0m, in \u001b[0;36m_euclidean_distances\u001b[0;34m(X, Y, X_norm_squared, Y_norm_squared, squared)\u001b[0m\n\u001b[1;32m    374\u001b[0m         YY \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32 \u001b[38;5;129;01mor\u001b[39;00m Y\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32:\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;66;03m# To minimize precision issues with float32, we compute the distance\u001b[39;00m\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;66;03m# matrix on chunks of X and Y upcast to float64\u001b[39;00m\n\u001b[0;32m--> 379\u001b[0m     distances \u001b[38;5;241m=\u001b[39m _euclidean_distances_upcast(X, XX, Y, YY)\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;66;03m# if dtype is already float64, no need to chunk and upcast\u001b[39;00m\n\u001b[1;32m    382\u001b[0m     distances \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m safe_sparse_dot(X, Y\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/metrics/pairwise.py:591\u001b[0m, in \u001b[0;36m_euclidean_distances_upcast\u001b[0;34m(X, XX, Y, YY, batch_size)\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    589\u001b[0m     YY_chunk \u001b[38;5;241m=\u001b[39m YY[:, y_slice]\n\u001b[0;32m--> 591\u001b[0m d \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m safe_sparse_dot(X_chunk, Y_chunk\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    592\u001b[0m d \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m XX_chunk\n\u001b[1;32m    593\u001b[0m d \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m YY_chunk\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/utils/extmath.py:211\u001b[0m, in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     ret \u001b[38;5;241m=\u001b[39m a \u001b[38;5;241m@\u001b[39m b\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m--> 211\u001b[0m     sparse\u001b[38;5;241m.\u001b[39missparse(a)\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(b)\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m dense_output\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(ret, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoarray\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    215\u001b[0m ):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/scipy/sparse/_base.py:1513\u001b[0m, in \u001b[0;36missparse\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1508\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m sparray\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m=\u001b[39m _spbase\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m\n\u001b[0;32m-> 1513\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21missparse\u001b[39m(x):\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Is `x` of a sparse array or sparse matrix type?\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \n\u001b[1;32m   1516\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;124;03m    False\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, _spbase)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "full_trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "LABEL_SIZE = 0.2\n",
    "\n",
    "# Split into labeled and unlabeled datasets (20% labeled)\n",
    "labeled_size = int(LABEL_SIZE * len(full_trainset))\n",
    "unlabeled_size = len(full_trainset) - labeled_size\n",
    "labeled_set, unlabeled_set = random_split(full_trainset, [labeled_size, unlabeled_size])\n",
    "\n",
    "print(f\"Labeled size: {labeled_size}, Unlabeled size: {unlabeled_size}\")\n",
    "\n",
    "\n",
    "# Compute ratio\n",
    "ratio = unlabeled_size / labeled_size\n",
    "\n",
    "# A simple heuristic: if ratio is large, update K-Means less often\n",
    "# For instance, if ratio = 1 means run every epoch, if ratio=5 means run every 5 epochs\n",
    "kmeans_update_frequency = max(1, int(ratio))\n",
    "\n",
    "print(f\"K-Means will be updated every {kmeans_update_frequency} epochs based on ratio={ratio:.2f}.\")\n",
    "\n",
    "# Create DataLoaders for labeled and unlabeled datasets\n",
    "labeled_loader = DataLoader(labeled_set, batch_size=32, shuffle=True)\n",
    "unlabeled_loader = DataLoader(unlabeled_set, batch_size=32, shuffle=True)\n",
    "\n",
    "# Load CIFAR-10 test set\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = DataLoader(testset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \n",
    "                      (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "model = CNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "K = 10  # number of clusters\n",
    "lambda_cluster = 0.5\n",
    "\n",
    "cluster_centers_torch = None  # will store cluster centers\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.eval()\n",
    "    \n",
    "    # Update K-Means only every kmeans_update_frequency epochs or if we have no cluster centers yet\n",
    "    if epoch % kmeans_update_frequency == 0 or cluster_centers_torch is None:\n",
    "        # ----- Run K-Means Clustering on Unlabeled Data -----\n",
    "        all_features = []\n",
    "        with torch.no_grad():\n",
    "            for unlabeled_batch in unlabeled_loader:\n",
    "                u_inputs = unlabeled_batch[0].to(device)\n",
    "                u_features, _ = model(u_inputs)\n",
    "                all_features.append(u_features.cpu().numpy())\n",
    "        \n",
    "        all_features = np.concatenate(all_features, axis=0)  # shape: [num_unlabeled_samples, feature_dim]\n",
    "\n",
    "        # Fit KMeans\n",
    "        kmeans = KMeans(n_clusters=K, random_state=42)\n",
    "        kmeans.fit(all_features)\n",
    "        cluster_centers = kmeans.cluster_centers_\n",
    "        cluster_centers_torch = torch.tensor(cluster_centers, dtype=torch.float32, device=device)\n",
    "        print(f\"Updated K-Means at epoch {epoch+1}\")\n",
    "\n",
    "    # Reset unlabeled loader iterator for this epoch\n",
    "    unlabeled_iter = iter(unlabeled_loader)\n",
    "\n",
    "    model.train()\n",
    "    running_supervised_loss = 0.0\n",
    "    running_cluster_loss = 0.0\n",
    "    \n",
    "    for x_l, y_l in labeled_loader:\n",
    "        x_l, y_l = x_l.to(device), y_l.to(device)\n",
    "        \n",
    "        # Get an unlabeled batch\n",
    "        try:\n",
    "            x_u, _ = next(unlabeled_iter)\n",
    "        except StopIteration:\n",
    "            unlabeled_iter = iter(unlabeled_loader)\n",
    "            x_u, _ = next(unlabeled_iter)\n",
    "        \n",
    "        x_u = x_u.to(device)\n",
    "        \n",
    "        # Compute supervised loss\n",
    "        f_l, logits_l = model(x_l)\n",
    "        supervised_loss = criterion(logits_l, y_l)\n",
    "        \n",
    "        # Compute cluster loss for the unlabeled batch:\n",
    "        f_u, _ = model(x_u)\n",
    "        \n",
    "        # Assign clusters by finding nearest center\n",
    "        with torch.no_grad():\n",
    "            dists = torch.cdist(f_u, cluster_centers_torch)\n",
    "            assignments = dists.argmin(dim=1)\n",
    "        \n",
    "        assigned_centers = cluster_centers_torch[assignments]\n",
    "        cluster_loss = ((f_u - assigned_centers)**2).mean()\n",
    "        \n",
    "        # Combine losses and backprop\n",
    "        total_loss = supervised_loss + lambda_cluster * cluster_loss\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_supervised_loss += supervised_loss.item()\n",
    "        running_cluster_loss += cluster_loss.item()\n",
    "    \n",
    "    avg_supervised_loss = running_supervised_loss / len(labeled_loader)\n",
    "    avg_cluster_loss = running_cluster_loss / len(labeled_loader)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: \"\n",
    "          f\"Supervised Loss = {avg_supervised_loss:.4f}, \"\n",
    "          f\"Cluster Loss = {avg_cluster_loss:.4f}\")\n",
    "\n",
    "# ----- Evaluation -----\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in testloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        _, logits = model(inputs)\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each epoch:\n",
    "    1. Supervised Phase:\n",
    "        - For each batch in labeled_loader:\n",
    "            a. Pass images through the labeled_labeled_model to get predictions (logits).\n",
    "            b. Compute supervised loss (cross-entropy).\n",
    "            c. Backpropagate and update labeled_labeled_model weights.\n",
    "\n",
    "    2. Unsupervised Phase:\n",
    "        a. Extract features for all images in unlabeled_loader.\n",
    "        b. Perform clustering (e.g., K-Means) on the features.\n",
    "        c. For each batch in unlabeled_loader:\n",
    "            - Compute clustering loss (distance to cluster centers).\n",
    "            - Backpropagate and update the feature extractor.\n",
    "\n",
    "    Log both supervised and unsupervised losses.\n",
    "    Evaluate on the test set to track performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
